{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.utils.benchmark\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(dynamic=True)\n",
    "def orthogonalize_QR(G: torch.Tensor):\n",
    "    \"\"\"Orthogonalize matrix using QR decomposition\"\"\"\n",
    "    Q, R = torch.linalg.qr(G)\n",
    "    return Q\n",
    "\n",
    "\n",
    "@torch.compile(dynamic=True)\n",
    "def orthogonalize_CQR(G):\n",
    "    \"\"\"Orthogonalize matrix using Cholesky QR decomposition\"\"\"\n",
    "    R, _ = torch.linalg.cholesky_ex(G.T @ G, upper=True)\n",
    "    Q = torch.linalg.solve_triangular(R, G, upper=True, left=False)\n",
    "    return Q\n",
    "\n",
    "\n",
    "@torch.compile(dynamic=True)\n",
    "def orthogonalize_NS(\n",
    "    G: torch.Tensor,\n",
    "    ns_iters: int = 5,\n",
    "    tuned_ns_consts: bool = True,\n",
    "    epsilon: float = 1e-8,\n",
    "):\n",
    "    \"\"\"Orthogonalize matrix using Newton-Schulz iteration\"\"\"\n",
    "    if tuned_ns_consts:\n",
    "        # Tuned constants\n",
    "        a, b, c = 3.4445, -4.7750, 2.0315\n",
    "    else:\n",
    "        # \"Vanilla\" constants\n",
    "        a, b, c = 2.0, -1.5, 0.5\n",
    "\n",
    "    # Convert to 16 bit\n",
    "    if G.device.type == \"cuda\":\n",
    "        G = G.to(torch.bfloat16)\n",
    "\n",
    "    # Normalize G so it doesn't blow up in iteration\n",
    "    X = G / (G.norm() + epsilon)\n",
    "    transposed = False\n",
    "\n",
    "    # If G has more rows than columns, transpose it\n",
    "    if X.size(0) > X.size(1):\n",
    "        X = X.T\n",
    "        transposed = True\n",
    "\n",
    "    for _ in range(ns_iters):\n",
    "        A = X @ X.T\n",
    "        B = b * A + c * (A @ A)\n",
    "        X = a * X + B @ X\n",
    "\n",
    "    if transposed:\n",
    "        X = X.T\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "@torch.compile(dynamic=True)\n",
    "def orthogonalize_SVD(G: torch.Tensor):\n",
    "    \"\"\"Orthogonalize matrix using SVD\"\"\"\n",
    "    U, _, Vt = torch.linalg.svd(G)\n",
    "    return U @ Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fcc3128e0b0>\n",
      "orthogonalize_QR(x)\n",
      "setup: from __main__ import orthogonalize_QR\n",
      "  Median: 5.13 ms\n",
      "  2 measurements, 100 runs per measurement, 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fcc312cec20>\n",
      "orthogonalize_CQR(x)\n",
      "setup: from __main__ import orthogonalize_CQR\n",
      "  Median: 1.38 ms\n",
      "  IQR:    0.00 ms (1.38 to 1.38)\n",
      "  8 measurements, 100 runs per measurement, 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fcc3c8b9ab0>\n",
      "orthogonalize_NS(x)\n",
      "setup: from __main__ import orthogonalize_NS\n",
      "  Median: 367.75 us\n",
      "  IQR:    4.98 us (365.73 to 370.71)\n",
      "  2677 measurements, 1 runs per measurement, 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fcc3c8b83d0>\n",
      "orthogonalize_SVD(x)\n",
      "setup: from __main__ import orthogonalize_SVD\n",
      "  Median: 61.12 ms\n",
      "  IQR:    0.08 ms (61.09 to 61.16)\n",
      "  17 measurements, 1 runs per measurement, 1 thread\n"
     ]
    }
   ],
   "source": [
    "# testing torch.utils.benchmark.Timer\n",
    "\n",
    "x = torch.randn(1024, 1024)\n",
    "x = x.to(device=\"cuda\")\n",
    "\n",
    "timer = torch.utils.benchmark.Timer(\n",
    "    stmt=\"orthogonalize_QR(x)\",\n",
    "    setup=\"from __main__ import orthogonalize_QR\",\n",
    "    globals={\"x\": x},\n",
    ")\n",
    "print(timer.blocked_autorange(min_run_time=1))\n",
    "\n",
    "timer = torch.utils.benchmark.Timer(\n",
    "    stmt=\"orthogonalize_CQR(x)\",\n",
    "    setup=\"from __main__ import orthogonalize_CQR\",\n",
    "    globals={\"x\": x},\n",
    ")\n",
    "print(timer.blocked_autorange(min_run_time=1))\n",
    "\n",
    "timer = torch.utils.benchmark.Timer(\n",
    "    stmt=\"orthogonalize_NS(x)\",\n",
    "    setup=\"from __main__ import orthogonalize_NS\",\n",
    "    globals={\"x\": x},\n",
    ")\n",
    "print(timer.blocked_autorange(min_run_time=1))\n",
    "\n",
    "timer = torch.utils.benchmark.Timer(\n",
    "    stmt=\"orthogonalize_SVD(x)\",\n",
    "    setup=\"from __main__ import orthogonalize_SVD\",\n",
    "    globals={\"x\": x},\n",
    ")\n",
    "print(timer.blocked_autorange(min_run_time=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_qr_and_ns(matrix_shapes, device=\"cuda\", ns_iters=5):\n",
    "    \"\"\"\n",
    "    Benchmark QR vs Newton-Schulz orthogonalization.\n",
    "    matrix_shapes: list of tuples, each tuple is a shape (m, n)\n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "    results = []\n",
    "\n",
    "    for shape in tqdm(matrix_shapes):\n",
    "        assert len(shape) == 2, \"Shape must be tuple of length 2\"\n",
    "\n",
    "        x = torch.randn(*shape, device=device)\n",
    "        label = f\"QR vs NS-{ns_iters} for device={device.type}\"\n",
    "        sub_label = f\"{shape[0]}x{shape[1]}\"\n",
    "\n",
    "        timer = torch.utils.benchmark.Timer(\n",
    "            stmt=\"orthogonalize_QR(x)\",\n",
    "            setup=\"from __main__ import orthogonalize_QR\",\n",
    "            globals={\"x\": x},\n",
    "            description=\"QR\",\n",
    "            label=label,\n",
    "            sub_label=sub_label,\n",
    "        )\n",
    "        qr_time = timer.blocked_autorange(min_run_time=1)\n",
    "\n",
    "        timer = torch.utils.benchmark.Timer(\n",
    "            stmt=\"orthogonalize_NS(x, ns_iters=ns_iters)\",\n",
    "            setup=\"from __main__ import orthogonalize_NS\",\n",
    "            globals={\"x\": x, \"ns_iters\": ns_iters},\n",
    "            description=\"NS\",\n",
    "            label=label,\n",
    "            sub_label=sub_label,\n",
    "        )\n",
    "        ns_time = timer.blocked_autorange(min_run_time=1)\n",
    "\n",
    "        results.extend((qr_time, ns_time))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(256, 256), (256, 1024), (1024, 256), (512, 512), (512, 2048), (2048, 512), (1024, 1024), (1024, 4096), (4096, 1024)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:36<00:00,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[- QR vs NS-5 for device=cpu -]\n",
      "                 |   QR  |   NS\n",
      "1 threads: --------------------\n",
      "      256x256    |    1  |    5\n",
      "      256x1024   |    5  |   15\n",
      "      1024x256   |    6  |   15\n",
      "      512x512    |   10  |   38\n",
      "      512x2048   |   34  |  120\n",
      "      2048x512   |   37  |  121\n",
      "      1024x1024  |   67  |  310\n",
      "      1024x4096  |  240  |  930\n",
      "      4096x1024  |  261  |  940\n",
      "\n",
      "Times are in milliseconds (ms).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Benchmark for CPU\n",
    "# Use smaller matrices for CPU to avoid long run times\n",
    "base_dim = [256, 512, 1024]\n",
    "matrix_shapes = []\n",
    "for n in base_dim:\n",
    "    matrix_shapes.extend([(n, n), (n, 4 * n), (4 * n, n)])\n",
    "print(matrix_shapes)\n",
    "\n",
    "results = benchmark_qr_and_ns(matrix_shapes, device=\"cpu\", ns_iters=5)\n",
    "\n",
    "compare = torch.utils.benchmark.Compare(results)\n",
    "compare.trim_significant_figures()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1024, 1024), (1024, 4096), (4096, 1024), (2048, 2048), (2048, 8192), (8192, 2048), (4096, 4096), (4096, 16384), (16384, 4096), (8192, 8192), (8192, 32768), (32768, 8192), (16384, 16384), (16384, 65536), (65536, 16384)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:48<00:00,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------ QR vs NS-5 for device=cuda -----]\n",
      "                   |     QR    |     NS  \n",
      "1 threads: ------------------------------\n",
      "      1024x1024    |     5130  |      339\n",
      "      1024x4096    |     6810  |      535\n",
      "      4096x1024    |    11400  |      565\n",
      "      2048x2048    |    12200  |      720\n",
      "      2048x8192    |    17000  |     2100\n",
      "      8192x2048    |    35000  |     2200\n",
      "      4096x4096    |    38100  |     5100\n",
      "      4096x16384   |    56700  |    10000\n",
      "      16384x4096   |   120000  |    10000\n",
      "      8192x8192    |   134000  |    40000\n",
      "      8192x32768   |   245000  |   100000\n",
      "      32768x8192   |   450000  |   100000\n",
      "      16384x16384  |   630000  |   300000\n",
      "      16384x65536  |  1457000  |  1019000\n",
      "      65536x16384  |  2620000  |  1008000\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Benchmark for GPU\n",
    "base_dim = [1024, 2048, 4096, 8192, 16384]\n",
    "matrix_shapes = []\n",
    "for n in base_dim:\n",
    "    matrix_shapes.extend([(n, n), (n, 4 * n), (4 * n, n)])\n",
    "print(matrix_shapes)\n",
    "\n",
    "results = benchmark_qr_and_ns(matrix_shapes, device=\"cuda\", ns_iters=5)\n",
    "\n",
    "compare = torch.utils.benchmark.Compare(results)\n",
    "compare.trim_significant_figures()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1024x1024\tQR=   5130\tNS=    339\tratio=15.14\n",
      "   1024x4096\tQR=   6807\tNS=    536\tratio=12.71\n",
      "   4096x1024\tQR=  11411\tNS=    565\tratio=20.18\n",
      "   2048x2048\tQR=  12238\tNS=    724\tratio=16.90\n",
      "   2048x8192\tQR=  17055\tNS=   2097\tratio=8.13\n",
      "   8192x2048\tQR=  34966\tNS=   2214\tratio=15.79\n",
      "   4096x4096\tQR=  38107\tNS=   5124\tratio=7.44\n",
      "  4096x16384\tQR=  56931\tNS=  14774\tratio=3.85\n",
      "  16384x4096\tQR= 116737\tNS=  15204\tratio=7.68\n",
      "   8192x8192\tQR= 134161\tNS=  38114\tratio=3.52\n",
      "  8192x32768\tQR= 245070\tNS= 114569\tratio=2.14\n",
      "  32768x8192\tQR= 448316\tNS= 114732\tratio=3.91\n",
      " 16384x16384\tQR= 633337\tNS= 297215\tratio=2.13\n",
      " 16384x65536\tQR=1457101\tNS=1019387\tratio=1.43\n",
      " 65536x16384\tQR=2619854\tNS=1007954\tratio=2.60\n"
     ]
    }
   ],
   "source": [
    "# Print ratio of times\n",
    "# This assumes that the results are in pairs, first QR then NS\n",
    "for i in range(len(results) // 2):\n",
    "    qr_result = results[i * 2]\n",
    "    ns_result = results[i * 2 + 1]\n",
    "    assert qr_result.task_spec.sub_label == ns_result.task_spec.sub_label\n",
    "\n",
    "    shape = qr_result.task_spec.sub_label\n",
    "    qr_time = 1e6 * qr_result.mean\n",
    "    ns_time = 1e6 * ns_result.mean\n",
    "    ratio = qr_time / ns_time\n",
    "    print(f\"{shape:>12}\\tQR={qr_time:7.0f}\\tNS={ns_time:7.0f}\\tratio={ratio:2.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_qr(matrix_shapes, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Benchmark QR vs Cholesky QR orthogonalization.\n",
    "    matrix_shapes: list of tuples, each tuple is a shape (m, n)\n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "    results = []\n",
    "\n",
    "    for shape in tqdm(matrix_shapes):\n",
    "        assert len(shape) == 2, \"Shape must be tuple of length 2\"\n",
    "\n",
    "        x = torch.randn(*shape, device=device)\n",
    "        label = f\"QR vs CQR for device={device.type}\"\n",
    "        sub_label = f\"{shape[0]}x{shape[1]}\"\n",
    "\n",
    "        timer = torch.utils.benchmark.Timer(\n",
    "            stmt=\"orthogonalize_QR(x)\",\n",
    "            setup=\"from __main__ import orthogonalize_QR\",\n",
    "            globals={\"x\": x},\n",
    "            description=\"QR\",\n",
    "            label=label,\n",
    "            sub_label=sub_label,\n",
    "        )\n",
    "        qr_time = timer.blocked_autorange(min_run_time=1)\n",
    "\n",
    "        timer = torch.utils.benchmark.Timer(\n",
    "            stmt=\"orthogonalize_CQR(x)\",\n",
    "            setup=\"from __main__ import orthogonalize_CQR\",\n",
    "            globals={\"x\": x},\n",
    "            description=\"CQR\",\n",
    "            label=label,\n",
    "            sub_label=sub_label,\n",
    "        )\n",
    "        cqr_time = timer.blocked_autorange(min_run_time=1)\n",
    "\n",
    "        results.extend((qr_time, cqr_time))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(256, 256), (256, 1024), (1024, 256), (512, 512), (512, 2048), (2048, 512), (1024, 1024), (1024, 4096), (4096, 1024)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:24<00:00,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-- QR vs CQR for device=cpu -]\n",
      "                 |   QR  |  CQR\n",
      "1 threads: --------------------\n",
      "      256x256    |    1  |    1\n",
      "      256x1024   |    5  |   17\n",
      "      1024x256   |    6  |    3\n",
      "      512x512    |   10  |    7\n",
      "      512x2048   |   35  |  100\n",
      "      2048x512   |   37  |   23\n",
      "      1024x1024  |   68  |   50\n",
      "      1024x4096  |  231  |  762\n",
      "      4096x1024  |  262  |  174\n",
      "\n",
      "Times are in milliseconds (ms).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Benchmark for CPU\n",
    "# Use smaller matrices for CPU to avoid long run times\n",
    "base_dim = [256, 512, 1024]\n",
    "matrix_shapes = []\n",
    "for n in base_dim:\n",
    "    matrix_shapes.extend([(n, n), (n, 4 * n), (4 * n, n)])\n",
    "print(matrix_shapes)\n",
    "\n",
    "results = benchmark_qr(matrix_shapes, device=\"cpu\")\n",
    "\n",
    "compare = torch.utils.benchmark.Compare(results)\n",
    "compare.trim_significant_figures()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1024, 1024), (1024, 4096), (4096, 1024), (2048, 2048), (2048, 8192), (8192, 2048), (4096, 4096), (4096, 16384), (16384, 4096), (8192, 8192), (8192, 32768), (32768, 8192), (16384, 16384), (16384, 65536), (65536, 16384)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:19<00:00,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--- QR vs CQR for device=cuda ----]\n",
      "                   |   QR   |   CQR \n",
      "1 threads: -------------------------\n",
      "      1024x1024    |     5  |      1\n",
      "      1024x4096    |     7  |      9\n",
      "      4096x1024    |    11  |      2\n",
      "      2048x2048    |    12  |      4\n",
      "      2048x8192    |    17  |     30\n",
      "      8192x2048    |    35  |      5\n",
      "      4096x4096    |    38  |     11\n",
      "      4096x16384   |    56  |    156\n",
      "      16384x4096   |   117  |     27\n",
      "      8192x8192    |   130  |     57\n",
      "      8192x32768   |   246  |   1104\n",
      "      32768x8192   |   451  |    200\n",
      "      16384x16384  |   637  |    400\n",
      "      16384x65536  |  1457  |  14940\n",
      "      65536x16384  |  2644  |   1408\n",
      "\n",
      "Times are in milliseconds (ms).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Benchmark for GPU\n",
    "base_dim = [1024, 2048, 4096, 8192, 16384]\n",
    "matrix_shapes = []\n",
    "for n in base_dim:\n",
    "    matrix_shapes.extend([(n, n), (n, 4 * n), (4 * n, n)])\n",
    "print(matrix_shapes)\n",
    "\n",
    "results = benchmark_qr(matrix_shapes, device=\"cuda\")\n",
    "\n",
    "compare = torch.utils.benchmark.Compare(results)\n",
    "compare.trim_significant_figures()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1024x1024\tQR=   5129\tCQR=   1382\tratio=3.71\n",
      "   1024x4096\tQR=   6849\tCQR=   9173\tratio=0.75\n",
      "   4096x1024\tQR=  11406\tCQR=   1655\tratio=6.89\n",
      "   2048x2048\tQR=  12195\tCQR=   3532\tratio=3.45\n",
      "   2048x8192\tQR=  17024\tCQR=  30303\tratio=0.56\n",
      "   8192x2048\tQR=  34889\tCQR=   5440\tratio=6.41\n",
      "   4096x4096\tQR=  38003\tCQR=  11342\tratio=3.35\n",
      "  4096x16384\tQR=  56436\tCQR= 156031\tratio=0.36\n",
      "  16384x4096\tQR= 116669\tCQR=  26653\tratio=4.38\n",
      "   8192x8192\tQR= 134117\tCQR=  56549\tratio=2.37\n",
      "  8192x32768\tQR= 246151\tCQR=1104199\tratio=0.22\n",
      "  32768x8192\tQR= 450815\tCQR= 179505\tratio=2.51\n",
      " 16384x16384\tQR= 637387\tCQR= 394717\tratio=1.61\n",
      " 16384x65536\tQR=1456792\tCQR=14937753\tratio=0.10\n",
      " 65536x16384\tQR=2643666\tCQR=1408217\tratio=1.88\n"
     ]
    }
   ],
   "source": [
    "# Print ratio of times\n",
    "# This assumes that the results are in pairs\n",
    "for i in range(len(results) // 2):\n",
    "    qr_result = results[i * 2]\n",
    "    cqr_result = results[i * 2 + 1]\n",
    "    assert qr_result.task_spec.sub_label == cqr_result.task_spec.sub_label\n",
    "\n",
    "    shape = qr_result.task_spec.sub_label\n",
    "    qr_time = 1e6 * qr_result.mean\n",
    "    cqr_time = 1e6 * cqr_result.mean\n",
    "    ratio = qr_time / cqr_time\n",
    "    print(f\"{shape:>12}\\tQR={qr_time:7.0f}\\tCQR={cqr_time:7.0f}\\tratio={ratio:2.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(dynamic=True)\n",
    "def simulate_muon(G):\n",
    "    a, b, c = 3.4445, -4.7750, 2.0315\n",
    "    X = G.bfloat16()\n",
    "    X /= X.norm() + 1e-8\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    for _ in range(5):\n",
    "        A = X @ X.T\n",
    "        B = b * A + c * (A @ A)\n",
    "        X = a * X + B @ X\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "\n",
    "\n",
    "@torch.compile(dynamic=True)\n",
    "def simulate_dion(G, Q):\n",
    "    P = G @ Q\n",
    "    P = torch.linalg.qr(P)[0]\n",
    "    R = G.T @ P\n",
    "    M = G - P @ R.T\n",
    "    Q = R / (R.sum(dim=0, keepdim=True) + 1e-8)\n",
    "    return P @ Q.T\n",
    "\n",
    "\n",
    "@torch.compile(dynamic=True)\n",
    "def simulate_cholesky_dion(G, Q):\n",
    "    P = G @ Q\n",
    "    A = P.T @ P\n",
    "    # A = A + torch.eye(A.size(0), device=A.device)\n",
    "    L, info = torch.linalg.cholesky_ex(A, upper=True)\n",
    "    print(\"info\", info)\n",
    "    P = torch.linalg.solve_triangular(L, P, upper=True, left=False)\n",
    "    # A = P.T @ P\n",
    "    # L, _ = torch.linalg.cholesky_ex(A, upper=True)\n",
    "    # P = torch.linalg.solve_triangular(L, P, upper=True, left=False)\n",
    "    R = G.T @ P\n",
    "    M = G - P @ R.T\n",
    "    Q = R / (R.sum(dim=0, keepdim=True) + 1e-8)\n",
    "    return P @ Q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond G tensor(2524.2595, device='cuda:0')\n",
      "cond P tensor(103697.7578, device='cuda:0')\n",
      "cond G@G^T tensor(8432830., device='cuda:0')\n",
      "cond P@P^T tensor(1.3741e+08, device='cuda:0')\n",
      "cond I + P@P^T tensor(8750351., device='cuda:0')\n",
      "info tensor(1022, device='cuda:0', dtype=torch.int32)\n",
      "tensor(1307.8210, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(11.1049, device='cuda:0')\n",
      "tensor(1264734.7500, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "m, n = 1024, 1024\n",
    "r = min(m, n) // 1\n",
    "G = torch.randn(m, n).to(device=\"cuda\")\n",
    "Q = torch.randn(n, r).to(device=\"cuda\")\n",
    "\n",
    "P = G @ Q\n",
    "I = torch.eye(m, device=G.device)\n",
    "print(\"cond G\", torch.linalg.cond(G))\n",
    "print(\"cond P\", torch.linalg.cond(P))\n",
    "print(\"cond G@G^T\", torch.linalg.cond(G @ G.T))\n",
    "print(\"cond P@P^T\", torch.linalg.cond(P @ P.T))\n",
    "print(\"cond I + P@P^T\", torch.linalg.cond(I + P @ P.T))\n",
    "\n",
    "muon_update = simulate_muon(G)\n",
    "dion_update = simulate_dion(G, Q)\n",
    "cqr_dion_update = simulate_cholesky_dion(G, Q)\n",
    "\n",
    "\n",
    "def test_orthogonality(X):\n",
    "    return (X.T @ X - torch.eye(X.size(1), device=X.device)).norm()\n",
    "\n",
    "\n",
    "print((muon_update - dion_update).norm())\n",
    "print((dion_update - cqr_dion_update).norm())\n",
    "print(test_orthogonality(muon_update))\n",
    "print(test_orthogonality(dion_update))\n",
    "print(test_orthogonality(cqr_dion_update))\n",
    "print(G.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17.4587, device='cuda:0')\n",
      "tensor(18.2574, device='cuda:0')\n",
      "tensor(2186.3999, device='cuda:0')\n",
      "\n",
      "tensor(39.6709, device='cuda:0')\n",
      "tensor(39.2988, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "\n",
      "tensor(1.0003, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(2719.0166, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def rcqr(X):\n",
    "    m, n = X.shape\n",
    "    k = math.ceil(1.25 * n)\n",
    "    theta = torch.randn(k, m, device=X.device)\n",
    "    theta.mul_(1.0 / math.sqrt(k))\n",
    "    P = theta @ X\n",
    "    _, R = torch.linalg.qr(P, mode=\"r\")\n",
    "    # R, _ = torch.linalg.cholesky_ex(P.T @ P, upper=True)\n",
    "    Q = torch.linalg.solve_triangular(R, X, upper=True, left=False)\n",
    "\n",
    "    # A = Q.T @ Q\n",
    "    # L, info = torch.linalg.cholesky_ex(A, upper=True)\n",
    "    # Q = torch.linalg.solve_triangular(L, Q, upper=True, left=False)\n",
    "    return Q\n",
    "\n",
    "\n",
    "def scqr(X):\n",
    "    A = X.T @ X\n",
    "    A = A + torch.eye(A.size(0), device=A.device)\n",
    "    L, info = torch.linalg.cholesky_ex(A, upper=True)\n",
    "    Q = torch.linalg.solve_triangular(L, X, upper=True, left=False)\n",
    "    return Q\n",
    "\n",
    "\n",
    "print(torch.linalg.cond(rcqr(G)))\n",
    "print(torch.linalg.cond(rcqr(P)))\n",
    "print(torch.linalg.cond(rcqr(P @ P.T)))\n",
    "print()\n",
    "\n",
    "print(torch.linalg.cond(scqr(G)))\n",
    "print(torch.linalg.cond(scqr(P)))\n",
    "print(torch.linalg.cond(scqr(P @ P.T)))\n",
    "print()\n",
    "\n",
    "print(torch.linalg.cond(orthogonalize_QR(P)))\n",
    "print(torch.linalg.cond(orthogonalize_CQR(P)))\n",
    "print(torch.linalg.cond(orthogonalize_NS(P).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix shape (n, 4*n)\n",
    "param shape (4*n, n)\n",
    "colwise parallel (4*n/TP, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear (col, row)\n",
    "colwise parallel shard(0)\n",
    "rowwise parallel shard(1)\n",
    "fsdp shard(0) default\n",
    "\n",
    "linear (col, row)\n",
    "colwise (TP, DP)\n",
    "rowwise (DP, TP)\n",
    "\n",
    "paper (row, col)\n",
    "colwise (DP, TP) (n, 4n)\n",
    "rowwise (TP, DP) (4n, n)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2]) torch.Size([2, 2])\n",
      "tensor([[ 0.3131,  1.7820],\n",
      "        [ 1.7147,  0.8314],\n",
      "        [ 0.4297,  1.3404],\n",
      "        [-1.0639, -0.6761],\n",
      "        [-0.5937, -0.7386],\n",
      "        [ 0.3421, -1.6879],\n",
      "        [ 0.0020, -1.2224],\n",
      "        [ 1.4354, -0.8162]])\n",
      "tensor([[-0.1193,  0.5077],\n",
      "        [-0.6535,  0.1025],\n",
      "        [-0.1638,  0.3653],\n",
      "        [ 0.4055, -0.1117],\n",
      "        [ 0.2263, -0.1707],\n",
      "        [-0.1304, -0.5356],\n",
      "        [-0.0008, -0.3669],\n",
      "        [-0.5471, -0.3678]])\n",
      "tensor([[-2.6238, -0.7493],\n",
      "        [ 0.0000,  3.3338]])\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "a = torch.randn(4 * n, n)\n",
    "q, r = torch.linalg.qr(a)\n",
    "print(q.shape, r.shape)\n",
    "print(a)\n",
    "print(q)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q0 shape torch.Size([4, 2])\n",
      "r0 shape torch.Size([2, 2])\n",
      "q1 shape torch.Size([4, 2])\n",
      "r1 shape torch.Size([2, 2])\n",
      "q2 shape torch.Size([4, 2])\n",
      "r2 shape torch.Size([2, 2])\n",
      "r2 tensor([[4.2202, 2.3031],\n",
      "        [0.0000, 2.4578]])\n"
     ]
    }
   ],
   "source": [
    "m = torch.randn(16, 2)\n",
    "a = m.chunk(4, dim=0)\n",
    "\n",
    "qr0 = [torch.linalg.qr(chunk) for chunk in a]\n",
    "q0 = [qr.Q for qr in qr0]\n",
    "r0 = [qr.R for qr in qr0]\n",
    "print(\"q0 shape\", q0[0].shape)\n",
    "print(\"r0 shape\", r0[0].shape)\n",
    "\n",
    "r1 = torch.cat(r0, dim=0).chunk(2, dim=0)\n",
    "qr1 = [torch.linalg.qr(chunk) for chunk in r1]\n",
    "q1 = [qr.Q for qr in qr1]\n",
    "r1 = [qr.R for qr in qr1]\n",
    "print(\"q1 shape\", q1[0].shape)\n",
    "print(\"r1 shape\", r1[0].shape)\n",
    "\n",
    "r2 = torch.cat(r1, dim=0).chunk(1, dim=0)\n",
    "qr1 = [torch.linalg.qr(chunk) for chunk in r2]\n",
    "q2 = [qr.Q for qr in qr1]\n",
    "r2 = [qr.R for qr in qr1]\n",
    "print(\"q2 shape\", q2[0].shape)\n",
    "print(\"r2 shape\", r2[0].shape)\n",
    "\n",
    "print(\"r2\", r2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7684e-07)\n",
      "tensor(1.0133e-06)\n"
     ]
    }
   ],
   "source": [
    "def cholesky_qr(X):\n",
    "    R, _ = torch.linalg.cholesky_ex(X.T @ X, upper=True)\n",
    "    Q = torch.linalg.solve_triangular(R, X, upper=True, left=False)\n",
    "    return Q, R\n",
    "\n",
    "\n",
    "m = torch.randn(100, 8)\n",
    "Q, R = cholesky_qr(m)\n",
    "Q_, R_ = torch.linalg.qr(m)\n",
    "\n",
    "print((m - Q @ R).abs().max())\n",
    "print((m - Q_ @ R_).abs().max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
