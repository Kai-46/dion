
# — Model —
model_dim: 1024
n_layer: 16
n_head: 8
sequence_length: 1024
vocab_size: 50304   # (implicitly set in code)

# — Batching & Training —
batch_size: 1024
device_batch_size: 32
num_iterations: 6000

# — Learning‐rate schedule —
warmup_ratio: 0.0
warmdown_ratio: 0.2

# — Optimizer & Hyperparameters —

adam_lr: 0.002
mu: 0.95
beta: 0.9
weight_decay: 0.01
rank_fraction: 0.5
oversample: 1.25

# — Validation & Checkpointing —
val_loss_every: 125
val_tokens: 10485760
save_every: 0

# — Weights & Biases logging —
wandb_project_name: dion-ref
wandb_job_name: null
no_wandb: false

# — Distributed training —
dp_size: null      # data‐parallel mesh dimension (set to an int to enable)
fs_size: null      # FSDP mesh dimension
tp_size: null      # tensor‐parallel mesh dimension
opt_grad_sync: false

# — Miscellaneous flags —
debug: false
no_compile: false


optimizer: dion_cqr #muon_moonlight
scalar_opt: lion
muon_adjust_lr: spectral_norm
lr: 0.02
approx_method: rcqr
qr_warmup: 200