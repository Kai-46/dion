{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d850b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8949b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_qr(\n",
    "    A: torch.Tensor, block_size: int = 16\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Block wise Gram Schmidt QR\n",
    "    Vector projections across blocks can be done with matrix multiplication\n",
    "    If block size is small enough, the current block can remain in GPU cache\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    assert n % block_size == 0, \"n must be divisible by block_size\"\n",
    "    num_blocks = n // block_size\n",
    "\n",
    "    Q = A.clone()\n",
    "    R = torch.zeros((n, n), device=A.device, dtype=A.dtype)\n",
    "\n",
    "    # Iterate over all output blocks\n",
    "    for block_idx in range(num_blocks):\n",
    "        start_col = block_idx * block_size\n",
    "        end_col = start_col + block_size\n",
    "        block = Q[:, start_col:end_col]\n",
    "\n",
    "        # Previous blocks are already orthogonalized\n",
    "        # Batch orthogonalize current block with respect to previous blocks\n",
    "        for prev_block_idx in range(block_idx):\n",
    "            prev_start = prev_block_idx * block_size\n",
    "            prev_end = prev_start + block_size\n",
    "            prev_block = Q[:, prev_start:prev_end]\n",
    "\n",
    "            S = prev_block.T @ block\n",
    "            block -= prev_block @ S\n",
    "            R[prev_start:prev_end, start_col:end_col] = S\n",
    "\n",
    "        # Sequentially orthogonalize each vector in the current block\n",
    "        R_block = R[start_col:end_col, start_col:end_col]\n",
    "        for j in range(block_size):\n",
    "            # Subtract projection of each previous vector\n",
    "            for i in range(j):\n",
    "                dot = torch.dot(block[:, i], block[:, j])\n",
    "                block[:, j] -= dot * block[:, i]\n",
    "                R_block[i, j] = dot\n",
    "\n",
    "            # Normalize the current vector\n",
    "            norm = torch.linalg.norm(block[:, j])\n",
    "            block[:, j] = block[:, j] / norm\n",
    "            R_block[j, j] = norm\n",
    "\n",
    "    return Q, R\n",
    "\n",
    "\n",
    "def check_qr(A, Q, R):\n",
    "    A = A.to(dtype=torch.float64)\n",
    "    Q = Q.to(dtype=torch.float64)\n",
    "    R = R.to(dtype=torch.float64)\n",
    "\n",
    "    # Check that Q@R = A\n",
    "    err = A - Q @ R\n",
    "    err_rms_norm = torch.sqrt(torch.mean(err**2))\n",
    "    print(f\"RMS norm of A - Q@R: {err_rms_norm:.2e}\")\n",
    "    print(f\"Max absolute error: {torch.max(torch.abs(err)):.2e}\")\n",
    "\n",
    "    # Check that Q is orthogonal\n",
    "    I = torch.eye(Q.shape[1], device=Q.device, dtype=Q.dtype)\n",
    "    err = Q.T @ Q - I\n",
    "    rms_norm = torch.sqrt(torch.mean(err**2))\n",
    "    print(f\"RMS norm of Q^T@Q - I: {rms_norm:.2e}\")\n",
    "\n",
    "    # Check that R is upper triangular\n",
    "    R_triu = torch.triu(R)\n",
    "    print(f\"R is triangular: {torch.allclose(R, R_triu)}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "def check_ortho(Q: torch.Tensor):\n",
    "    # Check that Q is orthogonal\n",
    "    Q = Q.to(dtype=torch.float64)\n",
    "    I = torch.eye(Q.shape[1], device=Q.device, dtype=Q.dtype)\n",
    "    err = Q.T @ Q - I\n",
    "    rms_norm = torch.sqrt(torch.mean(err**2))\n",
    "    print(f\"RMS norm of Q^T@Q - I: {rms_norm:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55c3a659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number of A: 6.26e+03\n",
      "\n",
      "===Checking QR decomposition with torch.linalg.qr\n",
      "RMS norm of A - Q@R: 5.15e-07\n",
      "Max absolute error: 4.56e-06\n",
      "RMS norm of Q^T@Q - I: 2.21e-08\n",
      "R is triangular: True\n",
      "\n",
      "===Checking QR decomposition with flash_qr\n",
      "RMS norm of A - Q@R: 1.35e-07\n",
      "Max absolute error: 1.64e-06\n",
      "RMS norm of Q^T@Q - I: 5.51e-07\n",
      "R is triangular: True\n",
      "\n",
      "===Checking QR decomposition with flash_qr with bfloat16\n",
      "RMS norm of A - Q@R: 8.47e-03\n",
      "Max absolute error: 1.05e-01\n",
      "RMS norm of Q^T@Q - I: 3.22e-03\n",
      "R is triangular: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = 1024\n",
    "A = torch.randn(d, d, dtype=torch.float32, device=\"cuda\")\n",
    "# A = A @ A.T @ A\n",
    "# A = A / torch.sqrt(torch.mean(A**2))\n",
    "A_f16 = A.to(dtype=torch.bfloat16)\n",
    "A = A_f16.to(dtype=torch.float32)\n",
    "print(f\"Condition number of A: {torch.linalg.cond(A):.2e}\\n\")\n",
    "\n",
    "\n",
    "print(\"===Checking QR decomposition with torch.linalg.qr\")\n",
    "Q, R = torch.linalg.qr(A)\n",
    "check_qr(A, Q, R)\n",
    "\n",
    "print(\"===Checking QR decomposition with flash_qr\")\n",
    "Q, R = flash_qr(A, block_size=16)\n",
    "check_qr(A, Q, R)\n",
    "\n",
    "print(\"===Checking QR decomposition with flash_qr with bfloat16\")\n",
    "Q, R = flash_qr(A_f16, block_size=16)\n",
    "check_qr(A, Q, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22740477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_triangular(R: torch.Tensor, A: torch.Tensor) -> torch.Tensor:\n",
    "    # Solve the triangular system Q@R = A for Q\n",
    "    m, n = A.shape\n",
    "    assert R.shape[0] == R.shape[1], \"R must be square\"\n",
    "    assert R.shape[0] == n, \"Incompatible shapes for triangular solve\"\n",
    "\n",
    "    Q = A.clone()\n",
    "\n",
    "    for col in range(n):\n",
    "        for i in range(col):\n",
    "            Q[:, col] -= R[i, col] * Q[:, i]\n",
    "        Q[:, col] /= R[col, col]\n",
    "\n",
    "    return Q\n",
    "\n",
    "\n",
    "def block_solve_triangular(\n",
    "    R: torch.Tensor, A: torch.Tensor, block_size: int = 16\n",
    ") -> torch.Tensor:\n",
    "    # Solve the triangular system Q@R = A for Q\n",
    "    m, n = A.shape\n",
    "    assert R.shape[0] == R.shape[1], \"R must be square\"\n",
    "    assert R.shape[0] == n, \"Incompatible shapes for triangular solve\"\n",
    "    assert n % block_size == 0, \"n must be divisible by block_size\"\n",
    "    num_blocks = n // block_size\n",
    "\n",
    "    Q = A.clone()\n",
    "\n",
    "    for block_idx in range(num_blocks):\n",
    "        start_col = block_idx * block_size\n",
    "        end_col = start_col + block_size\n",
    "        block = Q[:, start_col:end_col].clone()\n",
    "\n",
    "        # Blockwise subtract previously solved values\n",
    "        for prev_block_idx in range(block_idx):\n",
    "            prev_start = prev_block_idx * block_size\n",
    "            prev_end = prev_start + block_size\n",
    "            prev_block = Q[:, prev_start:prev_end]\n",
    "            R_block = R[prev_start:prev_end, start_col:end_col]\n",
    "            block -= prev_block @ R_block\n",
    "\n",
    "        # Sequentially process the current block of R along the diagonal\n",
    "        R_block = R[start_col:end_col, start_col:end_col]\n",
    "        for j in range(block_size):\n",
    "            for i in range(j):\n",
    "                block[:, j] -= R_block[i, j] * block[:, i]\n",
    "            block[:, j] /= R_block[j, j]\n",
    "\n",
    "        Q[:, start_col:end_col] = block\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fce8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS norm of A - Q@R: 1.78e-07\n",
      "Max absolute error: 1.82e-06\n",
      "RMS norm of Q^T@Q - I: 8.31e-07\n",
      "R is triangular: True\n",
      "\n",
      "RMS norm of A - Q@R: 1.39e-07\n",
      "Max absolute error: 1.64e-06\n",
      "RMS norm of Q^T@Q - I: 8.22e-07\n",
      "R is triangular: True\n",
      "\n",
      "RMS norm of A - Q@R: 8.46e-03\n",
      "Max absolute error: 1.48e-01\n",
      "RMS norm of Q^T@Q - I: 1.73e-01\n",
      "R is triangular: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = 1024\n",
    "A = torch.randn(d, d, dtype=torch.float32, device=\"cuda\")\n",
    "# A = A @ A.T @ A\n",
    "# A = A / torch.sqrt(torch.mean(A**2))\n",
    "Q, R = torch.linalg.qr(A)\n",
    "\n",
    "# Solve using torch.linalg.solve_triangular\n",
    "print(\"===Checking solve_triangular\")\n",
    "Q_solve = torch.linalg.solve_triangular(R, A, upper=True, left=False)\n",
    "check_qr(A, Q_solve, R)\n",
    "\n",
    "# Solve in float32\n",
    "print(\"===Checking block_solve_triangular\")\n",
    "Q_solve = block_solve_triangular(R, A, block_size=16)\n",
    "check_qr(A, Q_solve, R)\n",
    "\n",
    "# Solve in bfloat16\n",
    "print(\"===Checking block_solve_triangular with bfloat16\")\n",
    "A = A.to(dtype=torch.bfloat16)\n",
    "R = R.to(dtype=torch.bfloat16)\n",
    "Q_solve = block_solve_triangular(R, A, block_size=16)\n",
    "check_qr(A, Q_solve, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8b74754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_orthogonalize(\n",
    "    A: torch.Tensor, SA: torch.Tensor, block_size: int = 16\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Fused QR and solve triangular. The full R matrix is never materialized.\n",
    "        A = input matrix or shard of the input matrix\n",
    "        SA = random sketch of the whole A matrix\n",
    "        block_size = number of columns to process at once\n",
    "    Return orthogonalized copy of A\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    k, n2 = SA.shape\n",
    "    assert n == n2, f\"Incompatible shapes A={A.shape} and SA={SA.shape}\"\n",
    "    assert n % block_size == 0, \"n must be divisible by block_size\"\n",
    "    num_blocks = n // block_size\n",
    "\n",
    "    Q = SA.clone()\n",
    "    A = A.clone()\n",
    "\n",
    "    # Iterate over all output blocks\n",
    "    for block_idx in range(num_blocks):\n",
    "        start_col = block_idx * block_size\n",
    "        end_col = start_col + block_size\n",
    "        Q_block = Q[:, start_col:end_col]\n",
    "        A_block = A[:, start_col:end_col]\n",
    "\n",
    "        # Previous blocks of Q are already orthogonalized\n",
    "        for prev_block_idx in range(block_idx):\n",
    "            prev_start = prev_block_idx * block_size\n",
    "            prev_end = prev_start + block_size\n",
    "            Q_prev = Q[:, prev_start:prev_end]\n",
    "            A_prev = A[:, prev_start:prev_end]\n",
    "\n",
    "            # Compute one block of the R matrix\n",
    "            R_block = Q_prev.T @ Q_block\n",
    "            # Subtract projection of previous vectors from current Q block\n",
    "            Q_block -= Q_prev @ R_block\n",
    "            # Update the output matrix\n",
    "            A_block -= A_prev @ R_block\n",
    "\n",
    "        # Sequentially orthogonalize each vector in the current block\n",
    "        for j in range(block_size):\n",
    "            # Subtract projection of each previous vector\n",
    "            for i in range(j):\n",
    "                dot = torch.dot(Q_block[:, i], Q_block[:, j])\n",
    "                Q_block[:, j] -= dot * Q_block[:, i]\n",
    "                A_block[:, j] -= dot * A_block[:, i]\n",
    "\n",
    "            # Normalize the current vector\n",
    "            norm = torch.linalg.norm(Q_block[:, j])\n",
    "            Q_block[:, j] = Q_block[:, j] / norm\n",
    "            A_block[:, j] = A_block[:, j] / norm\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8480ad7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Orthogonalize directly\n",
      "RMS norm of Q^T@Q - I: 4.61e-07\n",
      "\n",
      "Orthogonalize with random sketch\n",
      "RMS norm of Q^T@Q - I: 3.12e-02\n",
      "\n",
      "Orthogonalize with Householder QR\n",
      "RMS norm of Q^T@Q - I: 1.89e-08\n",
      "\n",
      "Orthogonalize directly in bfloat16\n",
      "RMS norm of Q^T@Q - I: 3.27e-03\n",
      "\n",
      "Orthogonalize with random sketch in bfloat16\n",
      "RMS norm of Q^T@Q - I: 3.12e-02\n"
     ]
    }
   ],
   "source": [
    "m, n = 1024, 1024\n",
    "k = math.ceil(1.5 * n)\n",
    "A = torch.randn(m, n, dtype=torch.float32, device=\"cuda\")\n",
    "# A = A @ A.T @ A\n",
    "# A = A / torch.sqrt(torch.mean(A**2))\n",
    "\n",
    "S = torch.empty(k, m, dtype=A.dtype, device=\"cuda\").normal_(std=math.sqrt(n))\n",
    "SA = S @ A\n",
    "\n",
    "print(\"\\nOrthogonalize directly\")\n",
    "Q, _ = flash_qr(A, block_size=16)\n",
    "check_ortho(Q)\n",
    "\n",
    "print(\"\\nOrthogonalize with random sketch\")\n",
    "Q = flash_orthogonalize(A, SA, block_size=16)\n",
    "check_ortho(Q)\n",
    "\n",
    "print(\"\\nOrthogonalize with Householder QR\")\n",
    "Q, _ = torch.linalg.qr(A)\n",
    "check_ortho(Q)\n",
    "\n",
    "A = A.to(dtype=torch.bfloat16)\n",
    "S = S.to(dtype=torch.bfloat16)\n",
    "SA = S @ A\n",
    "\n",
    "print(\"\\nOrthogonalize directly in bfloat16\")\n",
    "Q, _ = flash_qr(A, block_size=16)\n",
    "check_ortho(Q)\n",
    "\n",
    "print(\"\\nOrthogonalize with random sketch in bfloat16\")\n",
    "Q = flash_orthogonalize(A, SA, block_size=16)\n",
    "check_ortho(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
